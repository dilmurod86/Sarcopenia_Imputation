{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7821177,"sourceType":"datasetVersion","datasetId":4582426}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\nfile_path = '/kaggle/input/misseddatasarcopenia/cleaned_dataset(Current97).csv'  # Update this path\ndata = pd.read_csv(file_path)\n\n# Impute missing values in the predictor features using IterativeImputer\nimputer = IterativeImputer(max_iter=10, random_state=0)\npredictor_features = data.drop(columns=['sarcopenia_2'])  # Exclude target feature for imputation\nimputed_features = imputer.fit_transform(predictor_features)\n\n# Scale the imputed features\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(imputed_features)\n\n# Restore the scaled features to a DataFrame with column names\nscaled_df = pd.DataFrame(scaled_features, columns=predictor_features.columns)\n\n# Re-include the 'sarcopenia_2' column to the scaled DataFrame\nscaled_df['sarcopenia_2'] = data['sarcopenia_2']\n\n# Check if there are any missing 'sarcopenia_2' values\nif scaled_df['sarcopenia_2'].isnull().any():\n    # Split the dataset into training (where sarcopenia_2 is not missing) and to be imputed (where sarcopenia_2 is missing)\n    train_data = scaled_df.dropna(subset=['sarcopenia_2'])\n    X_train = train_data.drop(columns=['sarcopenia_2'])\n    y_train = train_data['sarcopenia_2']\n    \n    # Train an SVR model\n    svr = SVR()\n    svr.fit(X_train, y_train)\n    \n    # Predict missing 'sarcopenia_2' values\n    to_impute_data = scaled_df[scaled_df['sarcopenia_2'].isnull()]\n    X_to_impute = to_impute_data.drop(columns=['sarcopenia_2'])\n    imputed_sarcopenia_2 = svr.predict(X_to_impute)\n    \n    # Fill in the missing 'sarcopenia_2' values in the original dataset\n    scaled_df.loc[scaled_df['sarcopenia_2'].isnull(), 'sarcopenia_2'] = imputed_sarcopenia_2\nelse:\n    print(\"No missing 'sarcopenia_2' values to impute.\")\n\n# Inverse transform the features to get back to the original scale, excluding 'sarcopenia_2'\noriginal_features = scaler.inverse_transform(scaled_df.drop(columns=['sarcopenia_2']))\noriginal_df = pd.DataFrame(original_features, columns=predictor_features.columns)\n\n# Add 'sarcopenia_2' back to the dataframe\noriginal_df['sarcopenia_2'] = scaled_df['sarcopenia_2']\n\n# Save the dataset with original scale values\noutput_file_path = '/kaggle/working/original_scale_imputed_cleaned_dataset.csv'  # Update this path\noriginal_df.to_csv(output_file_path, index=False)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-12T07:13:37.246054Z","iopub.execute_input":"2024-03-12T07:13:37.246726Z","iopub.status.idle":"2024-03-12T07:14:09.441909Z","shell.execute_reply.started":"2024-03-12T07:13:37.246676Z","shell.execute_reply":"2024-03-12T07:14:09.440677Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"No missing 'sarcopenia_2' values to impute.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Multiple imputation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.experimental import enable_iterative_imputer  # Enables experimental features\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\nfile_path = '/kaggle/input/misseddatasarcopenia/cleaned_dataset(Current97).csv'  # Update this path as necessary\ndata = pd.read_csv(file_path)\n\n# Prepare the SVR model to be used in the imputer\nsvr_estimator = SVR()\n\n# Use the SVR model in the IterativeImputer\nimputer = IterativeImputer(estimator=svr_estimator, max_iter=10, random_state=0)\n\n# Impute missing values in the predictor features using IterativeImputer with SVR\npredictor_features = data.drop(columns=['sarcopenia_2'])  # Exclude the target feature for imputation\nimputed_features = imputer.fit_transform(predictor_features)\n\n# Scale the imputed features\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(imputed_features)\n\n# Restore the scaled features to a DataFrame with column names\nscaled_df = pd.DataFrame(scaled_features, columns=predictor_features.columns)\n\n# Re-include the 'sarcopenia_2' column to the scaled DataFrame\nscaled_df['sarcopenia_2'] = data['sarcopenia_2']\n\n# No need for further SVR modeling for 'sarcopenia_2' prediction since the focus is on feature imputation\n\n# Inverse transform the features to get back to the original scale, excluding 'sarcopenia_2'\noriginal_features = scaler.inverse_transform(scaled_df.drop(columns=['sarcopenia_2']))\noriginal_df = pd.DataFrame(original_features, columns=predictor_features.columns)\n\n# Add 'sarcopenia_2' back to the dataframe\noriginal_df['sarcopenia_2'] = scaled_df['sarcopenia_2']\n\n# Save the dataset with original scale values\noutput_file_path = '/kaggle/working/MI_SVR_imputed.csv'  # Update this path as necessary\noriginal_df.to_csv(output_file_path, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:29:21.246854Z","iopub.execute_input":"2024-03-30T04:29:21.247290Z","iopub.status.idle":"2024-03-30T04:29:32.121966Z","shell.execute_reply.started":"2024-03-30T04:29:21.247247Z","shell.execute_reply":"2024-03-30T04:29:32.120934Z"},"trusted":true},"execution_count":1,"outputs":[]}]}